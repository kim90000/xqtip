{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8542c3c87a04429a909254abc6ab19c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16d53ad71e2e49b0a7feba7549540bb7",
              "IPY_MODEL_ac18686845774132b7eaf3c8ae069a66",
              "IPY_MODEL_0695f2e2741e48e4b7c8053b3c6beda6"
            ],
            "layout": "IPY_MODEL_0c1403d8ad8a45da8635b608ca9df1be"
          }
        },
        "16d53ad71e2e49b0a7feba7549540bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1aacaacd92ce4e75a66ec7f24da56ffb",
            "placeholder": "​",
            "style": "IPY_MODEL_3a7bf7e2555c44b096dee80e4550a39a",
            "value": "tokenizer.model: 100%"
          }
        },
        "ac18686845774132b7eaf3c8ae069a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27ccd09c23ea42f1a2a63e7bdf6a22de",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_204b279ee71d47a1b3410a6f23f2f182",
            "value": 499723
          }
        },
        "0695f2e2741e48e4b7c8053b3c6beda6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9e4f35140194604878ea85bfb9228e0",
            "placeholder": "​",
            "style": "IPY_MODEL_d6c888a963984c849b287a9ba4252a33",
            "value": " 500k/500k [00:00&lt;00:00, 4.58MB/s]"
          }
        },
        "0c1403d8ad8a45da8635b608ca9df1be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1aacaacd92ce4e75a66ec7f24da56ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a7bf7e2555c44b096dee80e4550a39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27ccd09c23ea42f1a2a63e7bdf6a22de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "204b279ee71d47a1b3410a6f23f2f182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9e4f35140194604878ea85bfb9228e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c888a963984c849b287a9ba4252a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38c1f325599a4f0c8f4bc42dac07a32f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f5ea596592249d78a655beab59f39e2",
              "IPY_MODEL_431ea6da6e65471780ede9d8a8435ca9",
              "IPY_MODEL_7c8d7104b4e4497bb582fb549c47dfc0"
            ],
            "layout": "IPY_MODEL_30a5cfe66f3e4a44bb85a417eb4dc91b"
          }
        },
        "6f5ea596592249d78a655beab59f39e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6c57bac39144a9784d5d528746d4bf2",
            "placeholder": "​",
            "style": "IPY_MODEL_929ecf06c5104bb1b6f72a1e9f8b3969",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "431ea6da6e65471780ede9d8a8435ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75d76c7118aa469cb9b2b8e8f126f38b",
            "max": 1618,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71121b8a0ff741159285af541819d656",
            "value": 1618
          }
        },
        "7c8d7104b4e4497bb582fb549c47dfc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06b36434fbd6458d9c539a42a4587773",
            "placeholder": "​",
            "style": "IPY_MODEL_66c895e6fa27434386fff50108c308da",
            "value": " 1.62k/1.62k [00:00&lt;00:00, 168kB/s]"
          }
        },
        "30a5cfe66f3e4a44bb85a417eb4dc91b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c57bac39144a9784d5d528746d4bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "929ecf06c5104bb1b6f72a1e9f8b3969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75d76c7118aa469cb9b2b8e8f126f38b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71121b8a0ff741159285af541819d656": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06b36434fbd6458d9c539a42a4587773": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c895e6fa27434386fff50108c308da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36a8592f79ba4c53893e8fdd6e816f6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_464dca194baf4b138599bc6c9b02305b",
              "IPY_MODEL_f4e2c7c25e834c6d87b3273f8adefd88",
              "IPY_MODEL_8297d90f98ee4c25b6f6c6ced0b935a5"
            ],
            "layout": "IPY_MODEL_daaff4bb150e494e9172cdb932ef1172"
          }
        },
        "464dca194baf4b138599bc6c9b02305b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_113594bae89b40ffbcaf706f2b981b0f",
            "placeholder": "​",
            "style": "IPY_MODEL_1f6f153ce4d042289a5a13fade91c963",
            "value": "tokenizer.model: 100%"
          }
        },
        "f4e2c7c25e834c6d87b3273f8adefd88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_454b1298174842989fb671d41627eacf",
            "max": 499723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dca0e2c93dc54b399762e43f2f6c98f8",
            "value": 499723
          }
        },
        "8297d90f98ee4c25b6f6c6ced0b935a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_834bf99b7f0943f096cf974b4f9f73bb",
            "placeholder": "​",
            "style": "IPY_MODEL_8f1a4c0833c24ad586cca6a57a7e2896",
            "value": " 500k/500k [00:00&lt;00:00, 6.87MB/s]"
          }
        },
        "daaff4bb150e494e9172cdb932ef1172": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "113594bae89b40ffbcaf706f2b981b0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f6f153ce4d042289a5a13fade91c963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "454b1298174842989fb671d41627eacf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dca0e2c93dc54b399762e43f2f6c98f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "834bf99b7f0943f096cf974b4f9f73bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f1a4c0833c24ad586cca6a57a7e2896": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "935b375b5f044ae0929415d87972e4e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_266f36dd86b7420e8064005b12476e58",
              "IPY_MODEL_571e573f99354e97b7a6a53da006b40a",
              "IPY_MODEL_39a306d633ad46d0a2af1537fb118cd9"
            ],
            "layout": "IPY_MODEL_f84885e5a24f4431b84e50a6107fd75d"
          }
        },
        "266f36dd86b7420e8064005b12476e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7443624b561b456ea339c1f0b10606ab",
            "placeholder": "​",
            "style": "IPY_MODEL_3221f19b573047ef88304c8bea40aa21",
            "value": "tokenizer.json: 100%"
          }
        },
        "571e573f99354e97b7a6a53da006b40a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4626612f22924a7d8cd95e596527f70a",
            "max": 1842767,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a27bbf82ee0e4874afbcb76e2b1ec046",
            "value": 1842767
          }
        },
        "39a306d633ad46d0a2af1537fb118cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ecaa05a13b4326b45d85eb6e15ebb6",
            "placeholder": "​",
            "style": "IPY_MODEL_1dbdf5eb1d1e4f638ba6a6444998a78d",
            "value": " 1.84M/1.84M [00:00&lt;00:00, 9.53MB/s]"
          }
        },
        "f84885e5a24f4431b84e50a6107fd75d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7443624b561b456ea339c1f0b10606ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3221f19b573047ef88304c8bea40aa21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4626612f22924a7d8cd95e596527f70a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a27bbf82ee0e4874afbcb76e2b1ec046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13ecaa05a13b4326b45d85eb6e15ebb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dbdf5eb1d1e4f638ba6a6444998a78d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2974ee18f78b48658ad1092e7ec1ac13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91bd1557382f415dae84bb6487df170c",
              "IPY_MODEL_f1b4ddf1f2974951b61d6cf796f4987f",
              "IPY_MODEL_fc9debcc098f43eaa684bbc3c99acdb2"
            ],
            "layout": "IPY_MODEL_cb9078c1302040e380d2167dfd2be06d"
          }
        },
        "91bd1557382f415dae84bb6487df170c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15ed33a1d7b84a45bbdb7f53fd457da6",
            "placeholder": "​",
            "style": "IPY_MODEL_cf205628117b4571ba8be1bfa35cabc6",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f1b4ddf1f2974951b61d6cf796f4987f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d6f98cbecd1449a98e4e9a5def766ae",
            "max": 414,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f59778eff9c743e798e1975b7c42c9eb",
            "value": 414
          }
        },
        "fc9debcc098f43eaa684bbc3c99acdb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7efd131b5f6e4c03acf29abd4c730049",
            "placeholder": "​",
            "style": "IPY_MODEL_e693cd7fcde141aa9cb87910bc16d5c7",
            "value": " 414/414 [00:00&lt;00:00, 30.0kB/s]"
          }
        },
        "cb9078c1302040e380d2167dfd2be06d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15ed33a1d7b84a45bbdb7f53fd457da6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf205628117b4571ba8be1bfa35cabc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6f98cbecd1449a98e4e9a5def766ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f59778eff9c743e798e1975b7c42c9eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7efd131b5f6e4c03acf29abd4c730049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e693cd7fcde141aa9cb87910bc16d5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db58dacd725144ae908f37f25e193b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0542219fc1864ab49794e9f071a0a116",
              "IPY_MODEL_895bfdf931184034a66fa7047bafb24a",
              "IPY_MODEL_7ce070d83d534feba14ffb04b9f8dfda"
            ],
            "layout": "IPY_MODEL_2038535cbf184004bbcca091ecac3fa3"
          }
        },
        "0542219fc1864ab49794e9f071a0a116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c8fdba5a76f4a1a90c6e6575193fe66",
            "placeholder": "​",
            "style": "IPY_MODEL_df3b349c6e8d4ec4a364135a5352c210",
            "value": "config.json: 100%"
          }
        },
        "895bfdf931184034a66fa7047bafb24a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b3e5ac324ed49f4872622b17866805c",
            "max": 986,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfa2ecdcc0ef469ba9c3f929c0c0a119",
            "value": 986
          }
        },
        "7ce070d83d534feba14ffb04b9f8dfda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f829cb4b3264c92896ca55ed4fba959",
            "placeholder": "​",
            "style": "IPY_MODEL_cd5cd4709a3b4a10a1a340b7da39328c",
            "value": " 986/986 [00:00&lt;00:00, 115kB/s]"
          }
        },
        "2038535cbf184004bbcca091ecac3fa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c8fdba5a76f4a1a90c6e6575193fe66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df3b349c6e8d4ec4a364135a5352c210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b3e5ac324ed49f4872622b17866805c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa2ecdcc0ef469ba9c3f929c0c0a119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f829cb4b3264c92896ca55ed4fba959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd5cd4709a3b4a10a1a340b7da39328c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13a4325e421f4a9286220eca8ba17760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a40697a6d50f4e8a9e74217c1e99ce71",
              "IPY_MODEL_6b929520fe654858b7e24c4d03e5b20b",
              "IPY_MODEL_9d850c718c1c4aaaafc6d3c41bc9a623"
            ],
            "layout": "IPY_MODEL_61d32e567175426bb25a44b6902716fc"
          }
        },
        "a40697a6d50f4e8a9e74217c1e99ce71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06df93eb46fa45a2a2301c2bd897b68b",
            "placeholder": "​",
            "style": "IPY_MODEL_e03f7d9c95614eb88595bf5167abcc82",
            "value": "model.safetensors: 100%"
          }
        },
        "6b929520fe654858b7e24c4d03e5b20b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9fde3d7404043daa6994a3e81d4e5d8",
            "max": 2148119064,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce4b7dfdd86d475693b61cf7bac56ca8",
            "value": 2148119064
          }
        },
        "9d850c718c1c4aaaafc6d3c41bc9a623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_227cef4986fb4bd8957624316764ca82",
            "placeholder": "​",
            "style": "IPY_MODEL_a982ce08b6d34995af5b6b1390fc5b25",
            "value": " 2.15G/2.15G [00:43&lt;00:00, 56.5MB/s]"
          }
        },
        "61d32e567175426bb25a44b6902716fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06df93eb46fa45a2a2301c2bd897b68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e03f7d9c95614eb88595bf5167abcc82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9fde3d7404043daa6994a3e81d4e5d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce4b7dfdd86d475693b61cf7bac56ca8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "227cef4986fb4bd8957624316764ca82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a982ce08b6d34995af5b6b1390fc5b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token ْْْْْْْْْْْْْXXXXXXX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F4LUSztSDg7",
        "outputId": "ca6f68b6-ac9d-42ec-ef9c-f2cacb25944f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `read` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `read`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMP0gkcfR6-q",
        "outputId": "6818b1a7-8871-4da8-bed1-eb21d7f461e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_path = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# تحميل التوكنلايزر الأصلي لـ LLaMA-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# تحميل النموذج من safetensors\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",  # تحديد نوع البيانات إلى float16\n",
        "    device_map=\"auto\"  # تلقائيًا تحديد الجهاز (CPU أو GPU)\n",
        ")\n",
        "\n",
        "# اختبار النموذج\n",
        "input_ids = tokenizer(\"The capital of France is\", return_tensors=\"pt\").input_ids\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=1  # تحديد الحد الأقصى لعدد التوكنات الجديدة\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0]))"
      ]
    },
    {
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "input_ids = input_ids.to(device)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "cHFTCk5tTeJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "input_ids = input_ids.to(device)\n",
        "model_path = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# تحميل التوكنلايزر الأصلي لـ LLaMA-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# تحميل النموذج من safetensors\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",  # تحديد نوع البيانات إلى float16\n",
        "    device_map=\"auto\"  # تلقائيًا تحديد الجهاز (CPU أو GPU)\n",
        ")\n",
        "\n",
        "# اختبار النموذج\n",
        "input_ids = tokenizer(\"The capital of France is\", return_tensors=\"pt\").input_ids\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=1  # تحديد الحد الأقصى لعدد التوكنات الجديدة\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gj7vTj0ETgUL",
        "outputId": "24d0ac28-2173-4cac-bf09-770180140162"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-85b630b1dc2c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relaxml/Llama-2-7b-QTIP-2Bit\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Define device first\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_path = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# تحميل التوكنلايزر الأصلي لـ LLaMA-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# تحميل النموذج من safetensors\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",  # تحديد نوع البيانات إلى float16\n",
        "    device_map=\"auto\"  # تلقائيًا تحديد الجهاز (CPU أو GPU)\n",
        ")\n",
        "\n",
        "# Move model to device after it's defined\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# اختبار النموذج\n",
        "input_ids = tokenizer(\"The capital of France is\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to device after it's defined\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=1  # تحديد الحد الأقصى لعدد التوكنات الجديدة\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGf8FjbOT1RL",
        "outputId": "ed3b8c7a-921b-491c-daa5-1f4c1e1bfb73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_path = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# تحميل التوكنلايزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
        "\n",
        "# تحميل النموذج وتحديد أنه يعمل على GPU مع تقليل الدقة إلى fp16\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,  # نصف الدقة لتوفير الرام\n",
        "    device_map=\"auto\"           # يوزع النموذج على الـ GPU\n",
        ")\n",
        "\n",
        "# اختبار النموذج\n",
        "input_text = \"The capital of France is\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# تشغيل التوليد\n",
        "output = model.generate(input_ids)\n",
        "print(tokenizer.decode(output[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVwYFkjWUKzz",
        "outputId": "998d9077-be22-45f5-d203-42dcbf8bd0f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# التأكد من أن الـ GPU متاح\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# تحميل التوكنلايزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# تحميل النموذج بصيغة GPTQ مع إجبار استخدام الـ GPU\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    use_safetensors=True,\n",
        "    device=\"cuda\",          # إجبار التشغيل على الـ GPU\n",
        "    use_triton=False,       # جرب True أو False حسب الأداء\n",
        "    inject_fused_attention=False\n",
        ")\n",
        "\n",
        "# تمرير الإدخال للنموذج\n",
        "input_text = \"The capital of France is\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# تشغيل التوليد على الـ GPU\n",
        "output = model.generate(input_ids)\n",
        "print(tokenizer.decode(output[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "Hgm96ng-XFTV",
        "outputId": "e49f119d-0fac-4213-d277-f73a8a2423f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'auto_gptq'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-68f6ebe66f19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mauto_gptq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoGPTQForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relaxml/Llama-2-7b-QTIP-2Bit\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'auto_gptq'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"CUDA Version:\", torch.version.cuda)\n",
        "print(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2q3M2jUXFsn",
        "outputId": "396ab12c-a711-4c1c-c35b-725fb417617c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "CUDA Version: 12.4\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# التأكد من أن الـ GPU متاح\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# تحميل التوكنلايزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# تحميل النموذج بوضع 4-bit لتوفير VRAM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,  # استخدام نصف الدقة لتوفير VRAM\n",
        "    load_in_4bit=True,          # تشغيله بوضع 4-bit على الـ GPU\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# تجربة إدخال نص\n",
        "input_text = \"The capital of France is\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# تشغيل التوليد على الـ GPU\n",
        "output = model.generate(input_ids)\n",
        "print(tokenizer.decode(output[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "g2nEyjYIXOgl",
        "outputId": "0004dd41-d47e-4931-f619-e0678f633d2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'relaxml/Llama-2-7b-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b6eb43000176>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# تحميل التوكنلايزر\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# تحميل النموذج بوضع 4-bit لتوفير VRAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2020\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2021\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'relaxml/Llama-2-7b-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto_gptq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af4dJen9XiU-",
        "outputId": "789f4324-2661-4cae-8f33-648f044f12c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting auto_gptq\n",
            "  Downloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (1.3.0)\n",
            "Collecting datasets (from auto_gptq)\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (1.26.4)\n",
            "Collecting rouge (from auto_gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko (from auto_gptq)\n",
            "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (2.5.1+cu124)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (0.5.3)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (4.48.3)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (0.14.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from auto_gptq) (4.67.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto_gptq) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto_gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto_gptq) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->auto_gptq) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->auto_gptq)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->auto_gptq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->auto_gptq) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto_gptq) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto_gptq) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.31.0->auto_gptq) (0.21.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->auto_gptq) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets->auto_gptq)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->auto_gptq) (2.2.2)\n",
            "Collecting xxhash (from datasets->auto_gptq)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets->auto_gptq)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->auto_gptq) (3.11.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge->auto_gptq) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto_gptq) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto_gptq) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto_gptq) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto_gptq) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto_gptq) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto_gptq) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->auto_gptq) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.31.0->auto_gptq) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->auto_gptq) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto_gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto_gptq) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->auto_gptq) (2025.1)\n",
            "Downloading auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, rouge, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gekko, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, auto_gptq\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed auto_gptq-0.7.1 datasets-3.3.2 dill-0.3.8 gekko-1.2.1 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge-1.0.1 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from auto_gptq import AutoGPTQForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# التأكد من أن الـ GPU متاح\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# تحميل التوكنلايزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# تحميل النموذج بصيغة GPTQ\n",
        "model = AutoGPTQForCausalLM.from_quantized(\n",
        "    model_id,\n",
        "    use_safetensors=True,\n",
        "    device=\"cuda\",          # إجبار التشغيل على الـ GPU\n",
        "    use_triton=False,       # جرب True أو False حسب الأداء\n",
        "    inject_fused_attention=False\n",
        ")\n",
        "\n",
        "# تجربة إدخال نص\n",
        "input_text = \"The capital of France is\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# تشغيل التوليد على الـ GPU\n",
        "output = model.generate(input_ids)\n",
        "print(tokenizer.decode(output[0]))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "96Q-QprXXYX1",
        "outputId": "b25a8892-200d-45a6-df67-0459e2849c6f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/usr/local/lib/python3.11/dist-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd(cast_inputs=torch.float16)\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "Can't load tokenizer for 'relaxml/Llama-2-7b-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-39ef401c081e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# تحميل التوكنلايزر\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# تحميل النموذج بصيغة GPTQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2020\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2021\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'relaxml/Llama-2-7b-QTIP-2Bit'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'relaxml/Llama-2-7b-QTIP-2Bit' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "8542c3c87a04429a909254abc6ab19c3",
            "16d53ad71e2e49b0a7feba7549540bb7",
            "ac18686845774132b7eaf3c8ae069a66",
            "0695f2e2741e48e4b7c8053b3c6beda6",
            "0c1403d8ad8a45da8635b608ca9df1be",
            "1aacaacd92ce4e75a66ec7f24da56ffb",
            "3a7bf7e2555c44b096dee80e4550a39a",
            "27ccd09c23ea42f1a2a63e7bdf6a22de",
            "204b279ee71d47a1b3410a6f23f2f182",
            "d9e4f35140194604878ea85bfb9228e0",
            "d6c888a963984c849b287a9ba4252a33"
          ]
        },
        "id": "vN6I1WpLXfxV",
        "outputId": "ec9fbb00-91ee-485c-a981-f7c97ede98ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8542c3c87a04429a909254abc6ab19c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_path = \"relaxml/Llama-2-7b-QTIP-2Bit\"\n",
        "\n",
        "# تحميل التوكنلايزر الأصلي لـ LLaMA-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b\")\n",
        "\n",
        "# تحميل النموذج من safetensors\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=\"auto\",  # تحديد نوع البيانات إلى float16\n",
        "    device_map=\"auto\"  # تلقائيًا تحديد الجهاز (CPU أو GPU)\n",
        ")\n",
        "\n",
        "# اختبار النموذج\n",
        "input_ids = tokenizer(\"The capital of France is\", return_tensors=\"pt\").input_ids\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=1  # تحديد الحد الأقصى لعدد التوكنات الجديدة\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "metadata": {
        "id": "YGqsEHBQYR92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "pipe = pipeline(\"text-generation\", model=\"relaxml/Llama-2-7b-chat-E8P-2Bit\", device_map=\"cuda\")\n",
        "pipe(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sh6zNXzYaD9",
        "outputId": "ff9d6bb5-d35f-4fa1-910f-da671674b8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-E8P-2Bit\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-chat-E8P-2Bit\", device_map=\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "sayj7z0cZJgG",
        "outputId": "bc8b9e71-a77d-4e71-d282-5160a148be4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8433c86ce025>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relaxml/Llama-2-7b-chat-E8P-2Bit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relaxml/Llama-2-7b-chat-E8P-2Bit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackbone_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackboneConfigMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBackboneMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mchat_template_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeHintParsingException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_json_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_STANDARD_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m from .doc import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/chat_template_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2057\u001b[0m )\n\u001b[1;32m   2058\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m from torch import (\n\u001b[0m\u001b[1;32m   2060\u001b[0m     \u001b[0m__config__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__config__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0m__future__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m__future__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbernoulli\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbinomial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinomial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/beta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirichlet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDirichlet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_family\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExponentialFamily\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbroadcast_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/distributions/dirichlet.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0m_Dirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcentration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(cls, name, bases, attrs)\u001b[0m\n\u001b[1;32m    327\u001b[0m     \"\"\"\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         backward_fn = type(\n\u001b[1;32m    331\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Backward\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBackwardCFunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"_forward_cls\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Define device first\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-E8P-2Bit\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"relaxml/Llama-2-7b-chat-E8P-2Bit\",\n",
        "    torch_dtype=\"auto\",  # تحديد نوع البيانات إلى float16\n",
        "    device_map=\"auto\"  # تلقائيًا تحديد الجهاز (CPU أو GPU)\n",
        ")\n",
        "\n",
        "# Move model to device after it's defined\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Example input\n",
        "input_text = \"The capital of France is\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Move input_ids to device after it's defined\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# Generate output\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=1  # تحديد الحد الأقصى لعدد التوكنات الجديدة\n",
        ")\n",
        "\n",
        "# Print the decoded output\n",
        "print(tokenizer.decode(output[0]))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "38c1f325599a4f0c8f4bc42dac07a32f",
            "6f5ea596592249d78a655beab59f39e2",
            "431ea6da6e65471780ede9d8a8435ca9",
            "7c8d7104b4e4497bb582fb549c47dfc0",
            "30a5cfe66f3e4a44bb85a417eb4dc91b",
            "d6c57bac39144a9784d5d528746d4bf2",
            "929ecf06c5104bb1b6f72a1e9f8b3969",
            "75d76c7118aa469cb9b2b8e8f126f38b",
            "71121b8a0ff741159285af541819d656",
            "06b36434fbd6458d9c539a42a4587773",
            "66c895e6fa27434386fff50108c308da",
            "36a8592f79ba4c53893e8fdd6e816f6d",
            "464dca194baf4b138599bc6c9b02305b",
            "f4e2c7c25e834c6d87b3273f8adefd88",
            "8297d90f98ee4c25b6f6c6ced0b935a5",
            "daaff4bb150e494e9172cdb932ef1172",
            "113594bae89b40ffbcaf706f2b981b0f",
            "1f6f153ce4d042289a5a13fade91c963",
            "454b1298174842989fb671d41627eacf",
            "dca0e2c93dc54b399762e43f2f6c98f8",
            "834bf99b7f0943f096cf974b4f9f73bb",
            "8f1a4c0833c24ad586cca6a57a7e2896",
            "935b375b5f044ae0929415d87972e4e9",
            "266f36dd86b7420e8064005b12476e58",
            "571e573f99354e97b7a6a53da006b40a",
            "39a306d633ad46d0a2af1537fb118cd9",
            "f84885e5a24f4431b84e50a6107fd75d",
            "7443624b561b456ea339c1f0b10606ab",
            "3221f19b573047ef88304c8bea40aa21",
            "4626612f22924a7d8cd95e596527f70a",
            "a27bbf82ee0e4874afbcb76e2b1ec046",
            "13ecaa05a13b4326b45d85eb6e15ebb6",
            "1dbdf5eb1d1e4f638ba6a6444998a78d",
            "2974ee18f78b48658ad1092e7ec1ac13",
            "91bd1557382f415dae84bb6487df170c",
            "f1b4ddf1f2974951b61d6cf796f4987f",
            "fc9debcc098f43eaa684bbc3c99acdb2",
            "cb9078c1302040e380d2167dfd2be06d",
            "15ed33a1d7b84a45bbdb7f53fd457da6",
            "cf205628117b4571ba8be1bfa35cabc6",
            "1d6f98cbecd1449a98e4e9a5def766ae",
            "f59778eff9c743e798e1975b7c42c9eb",
            "7efd131b5f6e4c03acf29abd4c730049",
            "e693cd7fcde141aa9cb87910bc16d5c7",
            "db58dacd725144ae908f37f25e193b18",
            "0542219fc1864ab49794e9f071a0a116",
            "895bfdf931184034a66fa7047bafb24a",
            "7ce070d83d534feba14ffb04b9f8dfda",
            "2038535cbf184004bbcca091ecac3fa3",
            "7c8fdba5a76f4a1a90c6e6575193fe66",
            "df3b349c6e8d4ec4a364135a5352c210",
            "1b3e5ac324ed49f4872622b17866805c",
            "dfa2ecdcc0ef469ba9c3f929c0c0a119",
            "0f829cb4b3264c92896ca55ed4fba959",
            "cd5cd4709a3b4a10a1a340b7da39328c",
            "13a4325e421f4a9286220eca8ba17760",
            "a40697a6d50f4e8a9e74217c1e99ce71",
            "6b929520fe654858b7e24c4d03e5b20b",
            "9d850c718c1c4aaaafc6d3c41bc9a623",
            "61d32e567175426bb25a44b6902716fc",
            "06df93eb46fa45a2a2301c2bd897b68b",
            "e03f7d9c95614eb88595bf5167abcc82",
            "c9fde3d7404043daa6994a3e81d4e5d8",
            "ce4b7dfdd86d475693b61cf7bac56ca8",
            "227cef4986fb4bd8957624316764ca82",
            "a982ce08b6d34995af5b6b1390fc5b25"
          ]
        },
        "id": "Nyep5j1TZXHt",
        "outputId": "8c5873c6-3d0c-4ce5-be1a-dc7f038ec49e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38c1f325599a4f0c8f4bc42dac07a32f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36a8592f79ba4c53893e8fdd6e816f6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "935b375b5f044ae0929415d87972e4e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2974ee18f78b48658ad1092e7ec1ac13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/986 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db58dacd725144ae908f37f25e193b18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.15G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13a4325e421f4a9286220eca8ba17760"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-E8P-2Bit\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"relaxml/Llama-2-7b-chat-E8P-2Bit\", device_map=\"cuda\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wNnAtnhcA7-",
        "outputId": "fcff2f2e-979d-4f19-ebd8-ad34186d95e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Cornell-RelaxML/qtip.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g0YEgJxcBNP",
        "outputId": "a87a9872-beef-42c0-da60-2ea6782d1480"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qtip'...\n",
            "remote: Enumerating objects: 612, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 612 (delta 29), reused 27 (delta 25), pack-reused 554 (from 1)\u001b[K\n",
            "Receiving objects: 100% (612/612), 884.50 KiB | 16.38 MiB/s, done.\n",
            "Resolving deltas: 100% (363/363), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip/qtip-kernels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPEH-Ac2fHzI",
        "outputId": "cb1b9432-9c05-4145-e5ff-19d52329565d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip/qtip-kernels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSy80GZQfT9v",
        "outputId": "7ab1dcf6-14d9-4b2f-eca6-87bbaf46c8c4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating qtip_kernels_cuda.egg-info\n",
            "writing qtip_kernels_cuda.egg-info/PKG-INFO\n",
            "writing dependency_links to qtip_kernels_cuda.egg-info/dependency_links.txt\n",
            "writing top-level names to qtip_kernels_cuda.egg-info/top_level.txt\n",
            "writing manifest file 'qtip_kernels_cuda.egg-info/SOURCES.txt'\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:497: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "reading manifest file 'qtip_kernels_cuda.egg-info/SOURCES.txt'\n",
            "writing manifest file 'qtip_kernels_cuda.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:416: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n",
            "  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:426: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n",
            "  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\n",
            "building 'qtip_kernels' extension\n",
            "creating build/temp.linux-x86_64-cpython-311/src\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c src/inference.cu -o build/temp.linux-x86_64-cpython-311/src/inference.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -lineinfo -keep -std=c++17 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=qtip_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "ptxas info    : 2 bytes gmem, 16 bytes cmem[4]\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c src/qtip_torch.cu -o build/temp.linux-x86_64-cpython-311/src/qtip_torch.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -lineinfo -keep -std=c++17 --ptxas-options=-v -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=qtip_kernels -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(463)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"smemReduceGatherSize\"\u001b[0m was declared but never referenced\n",
            "      constexpr uint32_t smemReduceGatherSize = 2 * 1024 * sizeof(float4);\n",
            "                         ^\n",
            "          detected during instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=3072U]\"\u001b[0m \u001b[32mat line 66 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(225)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "          load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx, laneId, reg_cs_next, reg_cs2_next);\n",
            "                          ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=3072U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=3072U]\"\u001b[0m \u001b[32mat line 66 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(266)\u001b[0m: \u001b[01;35mwarning\u001b[0m #191-D: type qualifier is meaningless on cast type\n",
            "              load_reg_cs<R>((const uint16_t * __restrict__) compressed, weight_idx + (1 - ki % 2) * u16_per_tile_block, laneId, reg_cs_next, reg_cs2_next);\n",
            "                              ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=3072U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=3072U]\"\u001b[0m \u001b[32mat line 66 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(259)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"x_line\"\u001b[0m was declared but never referenced\n",
            "          uint32_t x_line;\n",
            "                   ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=3072U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=3072U]\"\u001b[0m \u001b[32mat line 66 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=3072U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=3072U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=3072U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 86 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 107 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 116 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=53248U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 125 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 135 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 144 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=16384U, N=1U, K=53248U]\"\u001b[0m \u001b[32mat line 153 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 173 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 182 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 191 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 200 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 209 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=16384U, N=1U, K=16384U]\"\u001b[0m \u001b[32mat line 218 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 228 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 237 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=4096U, N=1U, K=14336U]\"\u001b[0m \u001b[32mat line 246 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 256 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 265 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=14336U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 274 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 284 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 293 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 302 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 312 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 332 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 342 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 351 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 362 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 371 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 380 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=10240U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 389 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 399 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 408 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=57344U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 417 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 438 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 448 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=2U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 457 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 476 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 496 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 506 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 515 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 524 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 544 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 554 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=3U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 563 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=4096U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 583 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=11008U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 603 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=12288U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 613 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=22016U, N=1U, K=4096U]\"\u001b[0m \u001b[32mat line 622 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 631 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=8192U, N=1U, K=28672U]\"\u001b[0m \u001b[32mat line 651 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=28672U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 661 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01msrc/inference.cu(203)\u001b[0m: \u001b[01;35mwarning\u001b[0m #186-D: pointless comparison of unsigned integer with zero\n",
            "      uint32_t this_warp_k = (warpId < (tileCountK % (warps_per_block * 4)) / 4) ? k_per_block + 2 : k_per_block;\n",
            "                                     ^\n",
            "          detected during:\n",
            "            instantiation of \u001b[01m\"void kernel_decompress_matvec<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 466\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec_ptr<L,S,R,V,M,N,K>(float *, const uint32_t *, const half2 *, const half2 *, CUstream_st *) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 56 of src/qtip_torch.cu\u001b[0m\n",
            "            instantiation of \u001b[01m\"void decompress_matvec<L,S,R,V,M,N,K>(at::Tensor &, at::Tensor &, at::Tensor &, at::Tensor &) [with L=16U, S=9U, R=4U, V=1U, M=1024U, N=1U, K=8192U]\"\u001b[0m \u001b[32mat line 670 of src/qtip_torch.cu\u001b[0m\n",
            "\n",
            "ptxas qtip_torch.ptx, line 492; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 560; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 643; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 711; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 834; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 902; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 994; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1062; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1183; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1251; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1342; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1410; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1527; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1595; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1690; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1758; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 1939; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2007; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2088; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2156; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2657; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2725; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2802; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2870; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 2989; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3057; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3142; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3210; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3327; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3395; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3475; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3543; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3667; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3735; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3815; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 3883; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4058; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4126; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4201; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4269; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4774; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4842; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4919; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 4987; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5103; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5171; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5253; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5321; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5450; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5518; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5599; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5667; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5781; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5849; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5928; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 5996; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 6478; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 6546; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 6623; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 6691; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 6810; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 6878; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 6963; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7031; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7148; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7216; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7296; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7364; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7488; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7556; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7636; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7704; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7879; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 7947; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8022; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8090; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8581; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8653; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8740; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8812; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8925; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 8997; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9094; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9166; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9279; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9351; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9443; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9515; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9627; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9699; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9784; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 9856; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 10382; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 10452; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 10537; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 10607; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 10757; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 10827; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 10932; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11002; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11152; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11222; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11322; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11392; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11541; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11611; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11711; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 11781; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12266; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12334; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12417; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12485; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12602; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12670; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12763; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12831; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 12948; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13016; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13104; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13172; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13288; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13356; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13437; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13505; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 13964; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14036; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14123; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14195; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14308; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14380; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14477; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14549; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14662; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14734; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14826; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 14898; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15010; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15082; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15167; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15239; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15765; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15835; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15920; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 15990; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16140; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16210; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16315; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16385; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16535; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16605; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16705; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16775; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16924; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 16994; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 17094; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 17164; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 17648; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 17716; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 17799; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 17867; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 17984; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18052; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18145; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18213; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18330; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18398; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18486; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18554; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18670; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18738; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18819; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 18887; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 19376; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 19448; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 19527; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 19599; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20080; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20152; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20233; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20305; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20417; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20489; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20574; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20646; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20757; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20829; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20914; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 20986; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 21096; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 21168; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 21251; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 21323; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 21860; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 21930; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22012; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22082; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22231; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22301; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22399; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22469; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22631; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22701; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22794; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 22864; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23011; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23081; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23177; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23247; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23746; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23814; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23891; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 23959; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24075; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24143; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24225; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24293; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24422; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24490; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24571; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24639; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24753; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24821; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24900; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 24968; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 25428; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 25500; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 25587; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 25659; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 25772; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 25844; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 25941; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26013; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26126; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26198; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26290; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26362; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26474; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26546; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26631; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 26703; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27229; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27299; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27384; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27454; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27604; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27674; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27779; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27849; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 27999; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 28069; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 28169; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 28239; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 28388; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 28458; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 28558; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 28628; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29113; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29181; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29264; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29332; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29449; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29517; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29610; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29678; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29795; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29863; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 29951; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30019; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30135; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30203; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30284; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30352; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30765; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30837; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 30973; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31045; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31126; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31198; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31312; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31384; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31467; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31539; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31655; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31727; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31812; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31884; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 31991; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 32063; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 32534; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 32604; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 32781; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 32851; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 32933; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33003; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33154; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33224; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33315; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33385; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33538; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33608; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33701; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33771; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33913; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 33983; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 34399; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 34467; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 34608; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 34676; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 34753; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 34821; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 34939; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35007; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35086; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35154; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35274; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35342; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35423; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35491; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35600; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 35668; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36101; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36176; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36268; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36340; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36446; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36518; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36614; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36686; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36803; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36875; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 36973; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 37045; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 37145; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 37217; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 37306; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 37378; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 37850; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 37923; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38023; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38093; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38236; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38306; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38410; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38480; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38634; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38704; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38810; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 38880; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39015; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39085; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39182; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39252; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39672; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39743; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39829; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 39897; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40007; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40075; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40167; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40235; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40356; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40424; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40518; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40586; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40690; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40758; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40843; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 40911; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41330; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41405; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41495; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41567; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41687; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41759; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41849; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 41921; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42035; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42107; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42197; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42269; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42378; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42450; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42538; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 42610; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43088; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43161; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43259; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43329; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43479; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43549; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43647; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43717; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43866; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 43936; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44034; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44104; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44246; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44316; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44412; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44482; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44909; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 44980; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45063; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45131; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45254; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45322; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45405; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45473; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45588; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45656; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45739; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45807; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45917; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 45985; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46066; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46134; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46552; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46627; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46717; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46789; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46909; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 46981; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47071; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47143; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47257; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47329; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47419; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47491; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47600; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47672; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47760; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 47832; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48274; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48346; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48425; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48497; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48613; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48685; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48775; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48847; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 48961; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49033; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49118; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49190; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49311; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49383; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49468; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49540; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49711; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49783; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49862; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 49934; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50395; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50470; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50562; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50634; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50740; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50812; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50908; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 50980; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51097; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51169; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51267; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51339; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51439; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51511; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51600; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 51672; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52082; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52157; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52249; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52321; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52427; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52499; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52595; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52667; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52784; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52856; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 52954; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53026; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53126; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53198; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53287; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53359; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53769; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53844; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 53936; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54008; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54114; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54186; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54282; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54354; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54471; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54543; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54641; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54713; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54813; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54885; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 54974; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55046; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55482; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55554; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55641; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55713; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55826; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55898; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 55995; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56067; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56180; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56252; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56344; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56416; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56528; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56600; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56685; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 56757; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57217; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57289; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57376; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57448; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57561; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57633; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57730; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57802; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57915; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 57987; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 58079; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 58151; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 58263; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 58335; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 58420; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 58492; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59018; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59088; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59173; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59243; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59393; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59463; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59568; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59638; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59788; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59858; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 59958; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 60028; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 60177; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 60247; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 60347; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 60417; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 60902; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 60970; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61053; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61121; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61238; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61306; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61399; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61467; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61584; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61652; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61740; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61808; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61924; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 61992; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 62073; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 62141; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 62601; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 62673; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 62760; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 62832; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 62945; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63017; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63114; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63186; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63299; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63371; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63463; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63535; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63647; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63719; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63804; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 63876; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 64402; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 64472; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 64557; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 64627; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 64777; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 64847; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 64952; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65022; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65172; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65242; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65342; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65412; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65561; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65631; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65731; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 65801; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66286; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66354; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66437; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66505; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66622; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66690; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66783; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66851; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 66968; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 67036; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 67124; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 67192; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 67308; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 67376; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 67457; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 67525; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68029; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68101; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68184; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68256; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68390; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68462; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68556; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68628; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68734; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68806; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68898; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 68970; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69085; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69157; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69244; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69316; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69481; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69553; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69634; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 69706; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70177; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70249; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70336; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70408; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70521; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70593; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70690; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70762; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70875; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 70947; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71039; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71111; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71223; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71295; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71380; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71452; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71912; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 71984; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72071; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72143; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72256; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72328; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72425; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72497; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72610; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72682; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72774; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72846; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 72958; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73030; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73115; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73187; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73649; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73721; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73802; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73874; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 73986; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74058; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74143; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74215; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74326; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74398; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74483; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74555; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74665; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74737; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74820; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 74892; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 75452; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 75522; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 75599; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 75669; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76212; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76285; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76383; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76453; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76603; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76673; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76771; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76841; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 76990; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 77060; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 77158; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 77228; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 77370; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 77440; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 77536; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 77606; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78124; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78194; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78273; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78343; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78495; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78565; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78662; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78732; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78879; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 78949; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79042; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79112; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79292; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79362; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79459; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79529; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79721; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79791; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79868; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 79938; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 80482; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 80555; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 80655; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 80725; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 80868; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 80938; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81042; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81112; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81266; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81336; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81442; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81512; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81647; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81717; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81814; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 81884; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82356; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82429; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82529; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82599; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82742; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82812; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82916; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 82986; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83140; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83210; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83316; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83386; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83521; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83591; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83688; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 83758; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84230; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84303; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84403; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84473; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84616; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84686; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84790; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 84860; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85014; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85084; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85190; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85260; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85395; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85465; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85562; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 85632; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86134; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86204; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86289; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86359; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86509; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86579; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86684; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86754; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86904; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 86974; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 87074; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 87144; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 87293; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 87363; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 87463; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 87533; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88114; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88184; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88265; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88335; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88503; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88573; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88680; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88750; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88892; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 88962; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89065; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89135; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89311; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89381; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89481; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89551; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89736; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89806; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89885; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 89955; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 90524; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 90594; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 90679; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 90749; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 90899; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 90969; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91074; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91144; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91294; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91364; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91464; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91534; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91683; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91753; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91853; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 91923; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 92465; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 92535; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 92620; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 92690; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 92840; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 92910; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93015; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93085; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93235; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93305; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93405; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93475; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93624; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93694; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93794; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 93864; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 94413; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 94483; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 94565; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 94635; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 94784; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 94854; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 94952; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95022; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95184; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95254; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95347; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95417; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95564; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95634; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95730; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 95800; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 96326; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 96394; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 96469; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 96537; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97012; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97083; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97166; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97234; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97357; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97425; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97508; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97576; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97691; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97759; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97842; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 97910; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98020; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98088; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98169; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98237; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98694; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98762; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98839; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 98907; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99026; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99094; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99179; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99247; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99364; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99432; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99512; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99580; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99704; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99772; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99852; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 99920; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 100095; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 100163; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 100238; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 100306; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 100781; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 100852; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 100938; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101006; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101116; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101184; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101276; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101344; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101465; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101533; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101627; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101695; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101799; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101867; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 101952; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102020; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102440; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102511; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102597; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102665; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102775; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102843; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 102935; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103003; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103124; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103192; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103286; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103354; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103458; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103526; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103611; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 103679; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104099; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104170; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104256; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104324; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104434; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104502; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104594; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104662; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104783; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104851; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 104945; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105013; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105117; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105185; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105270; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105338; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105783; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105851; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 105934; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106002; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106119; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106187; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106280; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106348; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106465; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106533; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106621; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106689; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106805; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106873; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 106954; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 107022; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 107538; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 107606; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 107685; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 107753; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 107891; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 107959; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108049; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108117; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108227; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108295; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108383; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108451; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108562; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108630; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108725; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108793; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 108955; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109023; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109102; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109170; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109654; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109722; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109805; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109873; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 109990; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110058; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110151; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110219; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110336; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110404; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110492; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110560; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110676; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110744; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110825; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 110893; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111362; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111430; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111513; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111581; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111698; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111766; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111859; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 111927; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112044; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112112; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112200; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112268; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112384; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112452; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112533; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 112601; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113075; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113143; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113220; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113288; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113404; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113472; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113554; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113622; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113751; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113819; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113900; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 113968; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 114082; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 114150; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 114229; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas qtip_torch.ptx, line 114297; error   : Feature '.m16n8k16' requires .target sm_80 or higher\n",
            "ptxas fatal   : Ptx assembly aborted due to errors\n",
            "error: command '/usr/local/cuda/bin/nvcc' failed with exit code 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/qtip\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-oiM6slfivW",
        "outputId": "864ef58f-d9ee-434f-e150-94925019b1e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/qtip\n",
            "Collecting accelerate==0.34.2 (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: cuda_python==12.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (12.6.0)\n",
            "Collecting datasets==2.20.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting fast_hadamard_transform==1.0.4.post1 (from -r requirements.txt (line 4))\n",
            "  Downloading fast_hadamard_transform-1.0.4.post1.tar.gz (6.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting glog==0.3.1 (from -r requirements.txt (line 5))\n",
            "  Downloading glog-0.3.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huggingface_hub==0.24.0 (from -r requirements.txt (line 6))\n",
            "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lm_eval==0.4.4 (from -r requirements.txt (line 7))\n",
            "  Downloading lm_eval-0.4.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==2.1.2 (from -r requirements.txt (line 8))\n",
            "  Downloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy==1.14.1 (from -r requirements.txt (line 9))\n",
            "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setuptools==69.5.1 (from -r requirements.txt (line 10))\n",
            "  Downloading setuptools-69.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting torch==2.4.0 (from -r requirements.txt (line 11))\n",
            "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting tqdm==4.66.4 (from -r requirements.txt (line 12))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.45.2 (from -r requirements.txt (line 13))\n",
            "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.20.0->-r requirements.txt (line 3))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (0.70.16)\n",
            "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets==2.20.0->-r requirements.txt (line 3))\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.20.0->-r requirements.txt (line 3)) (3.11.13)\n",
            "Collecting ninja (from fast_hadamard_transform==1.0.4.post1->-r requirements.txt (line 4))\n",
            "  Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting python-gflags>=3.1 (from glog==0.3.1->-r requirements.txt (line 5))\n",
            "  Downloading python-gflags-3.1.2.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from glog==0.3.1->-r requirements.txt (line 5)) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.24.0->-r requirements.txt (line 6)) (4.12.2)\n",
            "Collecting evaluate (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting jsonlines (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r requirements.txt (line 7)) (2.10.2)\n",
            "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r requirements.txt (line 7)) (0.14.0)\n",
            "Collecting pybind11>=2.6.2 (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting pytablewriter (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r requirements.txt (line 7)) (1.6.1)\n",
            "Collecting sqlitedict (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tqdm-multiprocess (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r requirements.txt (line 7)) (0.23.0)\n",
            "Collecting word2number (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.4->-r requirements.txt (line 7)) (10.6.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r requirements.txt (line 11)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r requirements.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r requirements.txt (line 11)) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.4.0->-r requirements.txt (line 11)) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0->-r requirements.txt (line 11))\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.45.2->-r requirements.txt (line 13)) (2024.11.6)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers==4.45.2->-r requirements.txt (line 13))\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->-r requirements.txt (line 11)) (12.4.127)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.20.0->-r requirements.txt (line 3)) (1.18.3)\n",
            "INFO: pip is looking at multiple versions of peft to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting peft>=0.2.0 (from lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets==2.20.0->-r requirements.txt (line 3)) (2025.1.31)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.4->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.4->-r requirements.txt (line 7)) (3.9.1)\n",
            "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r requirements.txt (line 7)) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.4->-r requirements.txt (line 7)) (5.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.4->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.4->-r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.4.0->-r requirements.txt (line 11)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.20.0->-r requirements.txt (line 3)) (2025.1)\n",
            "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.4->-r requirements.txt (line 7))\n",
            "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.4.0->-r requirements.txt (line 11)) (1.3.0)\n",
            "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.4->-r requirements.txt (line 7)) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.4->-r requirements.txt (line 7)) (8.1.8)\n",
            "Downloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glog-0.3.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Downloading huggingface_hub-0.24.0-py3-none-any.whl (419 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lm_eval-0.4.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-69.5.1-py3-none-any.whl (894 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading ninja-1.11.1.3-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
            "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\n",
            "Downloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\n",
            "Downloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\n",
            "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\n",
            "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\n",
            "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fast_hadamard_transform, python-gflags, rouge-score, sqlitedict, word2number\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for fast_hadamard_transform (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for fast_hadamard_transform\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for fast_hadamard_transform\n",
            "  Building wheel for python-gflags (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-gflags: filename=python_gflags-3.1.2-py3-none-any.whl size=57370 sha256=90fac5825c7a30314ee53778dbb242aa9d453fd14fa6792d2ae0ecb88ea99acf\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/32/87/be6254803d81af495c135b8b662d4a076e7a91dc7766f05a25\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=c4630e1841ab23125e30c20f57f551bafdf0f3ad67610068b19f43be39ee6e48\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=b5c766f5edf26211b0e7f21d077c6dda6a05d795b0796c180336bcdbda733734\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=93fcd901bff8e51348fe6672e08426dcd6f9ae6dc4de57008f4e74fd6b05e96a\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\n",
            "Successfully built python-gflags rouge-score sqlitedict word2number\n",
            "Failed to build fast_hadamard_transform\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (fast_hadamard_transform)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#fe_import import model_from_hf_path\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# تحميل التوكنلايزر\n",
        "model_id = \"relaxml/Llama-2-7b-chat-E8P-2Bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# تحميل النموذج باستخدام الطريقة الخاصة بـ QTIP\n",
        "model = model_from_hf_path(model_id)\n",
        "\n",
        "import torch\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# تجربة النموذج مع إدخال نصي\n",
        "input_text = \"The capital of France is\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "# توليد النص\n",
        "with torch.no_grad():\n",
        "    output = model.generate(input_ids, max_new_tokens=50)\n",
        "\n",
        "# طباعة النتيجة\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RHCz0an8gH3I",
        "outputId": "0150719c-71fe-49cf-c12e-aeba6882dcca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_from_hf_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4f50effa5fc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# تحميل النموذج باستخدام الطريقة الخاصة بـ QTIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_hf_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_from_hf_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export CUDA_VISIBLE_DEVICES=\"\"\n"
      ],
      "metadata": {
        "id": "OALQCpy5hqRQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/qtip\")  # أضف مسار مكتبة QTIP إلى Python\n"
      ],
      "metadata": {
        "id": "V7XcvxBihq6o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lib.utils.unsafe_import import model_from_hf_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "5PyDjsVfh1jH",
        "outputId": "7a7ee8f2-450d-4676-95cd-4037c2be8331"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'glog'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-4aa433d76564>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsafe_import\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_from_hf_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/qtip/lib/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkernel_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmath_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qtip/lib/utils/data_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mglog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'glog'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install glog\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzgOwbyjh4Rf",
        "outputId": "633796f5-eb48-4a77-d3d3-b03c17e78dd2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting glog\n",
            "  Using cached glog-0.3.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting python-gflags>=3.1 (from glog)\n",
            "  Using cached python_gflags-3.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from glog) (1.17.0)\n",
            "Using cached glog-0.3.1-py2.py3-none-any.whl (7.8 kB)\n",
            "Installing collected packages: python-gflags, glog\n",
            "Successfully installed glog-0.3.1 python-gflags-3.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "export TORCH_CUDA_ARCH_LIST=\"7.5\"\n",
        "export TORCH_NVCC_FLAGS=\"--gpu-architecture=compute_75 --gpu-code=sm_75\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "W9bkJgXCh8W_",
        "outputId": "58bcf99e-2d5d-463e-f49e-1111292ce1d5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-18-035f81e3dbaf>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-035f81e3dbaf>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    export TORCH_CUDA_ARCH_LIST=\"7.5\"\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#from qtip.lib.utils.unsafe_import import model_from_hf_path\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# تحميل التوكنلايزر\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"relaxml/Llama-2-7b-chat-E8P-2Bit\")\n",
        "\n",
        "# تحميل النموذج بالطريقة الصحيحة لـ QTIP\n",
        "model = model_from_hf_path(\"relaxml/Llama-2-7b-chat-E8P-2Bit\")\n",
        "\n",
        "# تحديد الجهاز (إذا استمرت المشاكل، استخدم \"cpu\" مؤقتًا)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "MJlgwJSfh_xI",
        "outputId": "e5597792-8835-4f7a-adfc-76adde82a0f8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_from_hf_path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-534f9a97e68b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# تحميل النموذج بالطريقة الصحيحة لـ QTIP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_hf_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relaxml/Llama-2-7b-chat-E8P-2Bit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# تحديد الجهاز (إذا استمرت المشاكل، استخدم \"cpu\" مؤقتًا)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_from_hf_path' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from quantizer import load_quantized_model\n",
        "\n",
        "quant_dir = \"llama-70b_2bit_quip\"\n",
        "\n",
        "quant_model = load_quantized_model(quant_dir).cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(quant_dir)\n",
        "\n",
        "input_ids = tokenizer.encode(\"The capital of France is\", return_tensors=\"pt\").cuda()\n",
        "print(tokenizer.decode(quant_model.generate(input_ids, do_sample=True)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "acnHsqxziEBw",
        "outputId": "e3e6bd4f-8ca9-4a38-c510-f46e97141f41"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'quantizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ff2d665f0f5c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mquantizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_quantized_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquant_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"llama-70b_2bit_quip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'quantizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m eval.interactive_gen --hf_path relaxml/Llama-2-13b-QTIP-3Bit --max_new_tokens 256 --streaming"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4OnHOa5iVGY",
        "outputId": "1c2375ac-2196-4a35-9300-0220eb44be00"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/qtip/eval/interactive_gen.py\", line 8, in <module>\n",
            "    from model.cache_utils import StaticCache\n",
            "  File \"/content/qtip/model/cache_utils.py\", line 12, in <module>\n",
            "    from transformers.utils import is_hqq_available, is_quanto_available, is_torchdynamo_compiling, logging\n",
            "ImportError: cannot import name 'is_quanto_available' from 'transformers.utils' (/usr/local/lib/python3.11/dist-packages/transformers/utils/__init__.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qtip.lib.utils.unsafe_import import model_from_hf_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "gQDmxDp7iqKK",
        "outputId": "e2b32e51-105f-4146-c050-62de13a104d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'qtip_kernels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-19644238efe4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mqtip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsafe_import\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_from_hf_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/qtip/lib/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfinetune\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkernel_check\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmath_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qtip/lib/utils/data_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmatmul_had\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatmul_hadU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/qtip/lib/codebook/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mqtip_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m kernels = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3072\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'qtip_kernels'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjZFOh4_iqmI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}